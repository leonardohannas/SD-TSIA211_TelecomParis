{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report TP1 SD211\n",
    "### Made by: Jean Paul Saba, Caren Dib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "- $A\\mathbf{w} = \\mathbf{b}$\n",
    "- $\\mathbf{w} = \\begin{bmatrix} \\mathbf{w}_1 \\\\ \\mathbf{w}_0 \\\\ \\mathbf{w}_2 \\end{bmatrix}$, where $\\mathbf{w}_1$ and $\\mathbf{w}_2$ are vectors, and $\\mathbf{w}_0$ is a scalar.\n",
    "- $\\mathbf{b}_t = y(t)$\n",
    "- $(A\\mathbf{w})_t = \\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_1 + \\mathbf{w}_0 - y(t) \\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_2$\n",
    "\n",
    "We need to show that:\n",
    "$$ y(t) = \\frac{\\mathbf{w}_1^T \\tilde{\\mathbf{x}}(t) + \\mathbf{w}_0}{\\mathbf{w}_2^T \\tilde{\\mathbf{x}}(t) + 1} $$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "From $A\\mathbf{w}_t = \\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_1 + \\mathbf{w}_0 - y(t) \\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_2$, we replace $Aw$ by $b$ which is $y(t)$:\n",
    "\n",
    "$$y(t) = \\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_1 + \\mathbf{w}_0 - y(t) \\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_2$$\n",
    "\n",
    "we rearrange to isolate $y(t)$:\n",
    "\n",
    "$$ y(t) = \\frac{\\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_1 + \\mathbf{w}_0}{\\tilde{\\mathbf{x}}(t)^T\\mathbf{w}_2 + 1} $$\n",
    "\n",
    "and as $A^T B = B^T A$, then:\n",
    "\n",
    "$$ y(t) = \\frac{\\mathbf{w}_1^T\\tilde{\\mathbf{x}}(t) + \\mathbf{w}_0}{\\mathbf{w}_2^T\\tilde{\\mathbf{x}}(t) + 1} $$\n",
    "\n",
    "This is the required form of $y(t)$. Therefore, we have shown that if $A\\mathbf{w} = \\mathbf{b}$, then $y(t)$ is as stated in the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00927821  0.08309371 -0.03672704 ...  0.01980595 -0.03057174\n",
      " -0.01188614]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]\n",
    "\n",
    "x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780.8984793523193"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = A_test.dot(x)\n",
    "mse = mean_squared_error(b_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.71840515652645"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(100)\n",
    "ridge.fit(A, b)\n",
    "new_b = ridge.predict(A_test)\n",
    "new_mse = mean_squared_error(b_test, new_b)\n",
    "new_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error after using the ridge model (regularized) is less than the unregularized one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f1: w \\rightarrow \\frac{1}{2}||Aw-b||^2 + \\frac{\\lambda}{2}||w||^2$\n",
    "\n",
    "Let's break it down into two parts:\n",
    "* $\\nabla(\\frac{1}{2}||Aw-b||^2) = A^T(Aw-b)$\n",
    "* $\\nabla(\\frac{\\lambda}{2}||w||^2)=\\lambda w$\n",
    "so:\n",
    "\n",
    "$\\nabla f_1(w)=A^T(Aw-b) + \\lambda w$\n",
    "\n",
    "Let's calculate the second derivative of $f_1(w)$ to check the convexivity.\n",
    "\n",
    "$\\nabla ^2 f_1(w)=A^T A + \\lambda I$\n",
    "\n",
    "So $f_1(w)$ is convex if $A^T A + \\lambda I$ is positiveÂ semidefinite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size:  2.863243114925734e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.01238055,  0.05775866, -0.00111774, ...,  0.01579912,\n",
       "        -0.03571617,  0.0133528 ]),\n",
       " 97544,\n",
       " 0.9999369501661834)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_f1(w, A, b, lambda_reg):\n",
    "    term1 = A.T.dot(A.dot(w) - b)\n",
    "    term2 = lambda_reg * w\n",
    "    return term1 + term2\n",
    "\n",
    "def lipschitz_constant(A):\n",
    "    eigenvalues = np.linalg.eigvals(np.dot(A.T, A))\n",
    "    L = np.max(np.real(eigenvalues))\n",
    "    return L\n",
    "\n",
    "def gradient_descent(A, b, lambda_reg, gradient_tol):\n",
    "    alpha = 1 / lipschitz_constant(A)\n",
    "    print(\"Step size: \", alpha)\n",
    "    w = np.zeros(A.shape[1])\n",
    "    iterations = 0\n",
    "    norm_gradient = float('+inf')\n",
    "    \n",
    "    while norm_gradient > gradient_tol:\n",
    "        gradient = gradient_f1(w, A, b, lambda_reg)\n",
    "        norm_gradient = np.linalg.norm(gradient)\n",
    "        \n",
    "        w -= alpha * gradient\n",
    "        iterations += 1\n",
    "    \n",
    "    return w, iterations\n",
    "\n",
    "lambda_reg = 100\n",
    "gradient_tol = 1\n",
    "\n",
    "w_optimal, num_iterations = gradient_descent(A, b, lambda_reg, gradient_tol)\n",
    "\n",
    "norm_gradient_optimal = np.linalg.norm(gradient_f1(w_optimal, A, b, lambda_reg))\n",
    "\n",
    "(w_optimal, num_iterations, norm_gradient_optimal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose $ \\frac{1}{L} $ constant as the constant step size. The Lipschitz constant is employed in optimization algorithms, such as gradient descent, to ensure stability and convergence. In the context of convex optimization problems, the Lipschitz continuity of the gradient is a crucial property.\n",
    "\n",
    "For gradient descent, denoted as $L$, the Lipschitz constant is related to the smoothness of the objective function. The Lipschitz condition for the gradient is expressed as:\n",
    "\n",
    "$$ ||\\nabla f(x) - \\nabla f(y)|| \\leq L ||x - y|| $$\n",
    "\n",
    "where $f$ is the objective function, $\\nabla f(x)$ is the gradient of $f$ at point $x$, and $L$ is the Lipschitz constant. This condition bounds the rate at which the gradient can change across different points in the function space.\n",
    "\n",
    "In the provided code for gradient descent, the Lipschitz constant is used to set a step size ($\\alpha$) that ensures convergence without oscillations or divergence. The Lipschitz constant acts as an upper bound on the magnitude of the gradient, and choosing the step size inversely proportional to $L$ helps balance convergence speed and stability.\n",
    "\n",
    "It took 97k iterations to get to under 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the differentiable function $ f2(w) $ and the non-smooth, convex function $ g2(w) $:\n",
    "\n",
    "$$ f2(w) = \\frac{1}{2} \\|Aw - b\\|_2^2 $$\n",
    "\n",
    "$$ g2(w) = \\lambda \\|w\\|_1 $$\n",
    "\n",
    "The objective function can then be expressed as the sum of $ f2(w) $ and $ g2(w) $:\n",
    "\n",
    "$$ F2(w) = f2(w) + g2(w) $$\n",
    "\n",
    "The gradient of $ f2(w) $ is given by:\n",
    "\n",
    "$$ \\nabla f2(w) = A^T(Aw - b) $$\n",
    "\n",
    "The proximal operator of $ g2(w) $, denoted as $ \\text{prox}_{g2}(v) $, is the soft-thresholding operator:\n",
    "\n",
    "$$ \\text{prox}_{g2}(v) = \\text{sign}(v) \\cdot \\max(0, |v| - \\lambda) $$\n",
    "\n",
    "where $ \\text{sign}(v) $ is the sign function and $ \\lambda $ corresponds to the regularization parameter in the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal w: [[-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.03048354  0.03088203  0.00500326 ...  0.0079661   0.0026795\n",
      "   0.0064129 ]\n",
      " [ 0.00750956  0.00770429  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.00334578 -0.00353691  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.         -0.         ... -0.         -0.\n",
      "  -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_thresholding_operator(v, threshold):\n",
    "    return np.sign(v) * np.maximum(0, np.abs(v) - threshold)\n",
    "\n",
    "def proximal_gradient_method(A, b, lambda_val, epsilon=1e-6):\n",
    "    m, n = A.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    alpha = 1 / lipschitz_constant(A)\n",
    "    \n",
    "    gradient = np.dot(A.T, np.dot(A, w) - b)\n",
    "    norm_gradient = np.linalg.norm(gradient, ord=2) + 1000\n",
    "\n",
    "    while norm_gradient > epsilon:\n",
    "        gradient = np.dot(A.T, np.dot(A, w) - b)\n",
    "        w = w - alpha * gradient\n",
    "\n",
    "        w = soft_thresholding_operator(w, lambda_val * alpha)\n",
    "\n",
    "        new_norm_gradient = np.linalg.norm(gradient, ord=2)\n",
    "\n",
    "        if np.abs(new_norm_gradient - norm_gradient) < epsilon:\n",
    "            break\n",
    "        \n",
    "        norm_gradient = new_norm_gradient\n",
    "\n",
    "    return w\n",
    "\n",
    "lambda_val = 200\n",
    "\n",
    "result = proximal_gradient_method(A, b, lambda_val, epsilon=100)\n",
    "print(\"Optimal w:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suggest testing when the norm isn't decreasing much to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.          0.00166146  0.00091599 ... -0.          0.00127137\n",
      " -0.00195512]\n"
     ]
    }
   ],
   "source": [
    "def proximal_gradient_method_line_search(A, b, lambda_reg, tol=1e-4):\n",
    "    n_features = A.shape[1]\n",
    "    w = np.zeros(n_features)\n",
    "    alpha = 1.0 / lipschitz_constant(A)\n",
    "\n",
    "    while True:\n",
    "        gradient_smooth = np.dot(A.T, (np.dot(A, w) - b))\n",
    "        w_new = soft_thresholding_operator(w - alpha * gradient_smooth, alpha * lambda_reg)\n",
    "\n",
    "        while f2(A, b, w_new, lambda_reg) > f2(A, b, w, lambda_reg) + np.sum(gradient_smooth * (w_new - w)) + (1 / (2 * alpha)) * np.linalg.norm(w_new - w, 2) ** 2:\n",
    "            alpha /= 2\n",
    "\n",
    "        if np.linalg.norm(w_new - w, 2) < tol:\n",
    "            break\n",
    "\n",
    "        w = w_new\n",
    "\n",
    "    return w\n",
    "\n",
    "def f2(A, b, w, lambda_reg):\n",
    "    residual = np.dot(A, w) - b\n",
    "    return 0.5 * np.linalg.norm(residual, 2) ** 2 + lambda_reg * np.linalg.norm(w, 1)\n",
    "\n",
    "lambda_reg = 200\n",
    "\n",
    "result = proximal_gradient_method_line_search(A, b, lambda_reg)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm with fixed step size takes ages to converge, but the one with line search takes couple of seconds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
